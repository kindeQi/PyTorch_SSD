{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm_notebook\n",
    "from matplotlib import pyplot as plt\n",
    "from itertools import product\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.functional import F\n",
    "from torchvision import models\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# from fastai import transforms, model, dataset, conv_learner\n",
    "\n",
    "from PIL import ImageDraw, ImageFont\n",
    "from matplotlib import patches, patheffects\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from augmentation import SSDAugmentation\n",
    "\n",
    "from Config import Config\n",
    "from SSD_model import get_SSD_model, lr_find\n",
    "from VOC_data import VOC_dataset\n",
    "from SSDloss import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection_collate_fn(batch):\n",
    "    imgs, bboxes, labels = [], [], []\n",
    "    for i, b, l in batch:\n",
    "        imgs.append(i); bboxes.append(b); labels.append(l)\n",
    "    return torch.stack(imgs), bboxes, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\1-usc\\SSD_PyTorch\\SSD_model.py:162: UserWarning: nn.init.kaiming_uniform is now deprecated in favor of nn.init.kaiming_uniform_.\n",
      "  nn.init.kaiming_uniform(m.weight.data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success build ssd model\n"
     ]
    }
   ],
   "source": [
    "config = Config('local')\n",
    "ssd_model = get_SSD_model(1, config.vgg_weight_path, config.vgg_reduced_weight_path)\n",
    "# ssd_model.freeze_basenet()\n",
    "\n",
    "print('success build ssd model')\n",
    "\n",
    "train_dataset = VOC_dataset(config.voc2007_root, config.voc2007_trn_anno)\n",
    "\n",
    "# img, bbox, label = train_dataset[0]\n",
    "# img = img.unsqueeze(0)\n",
    "\n",
    "# conf_pred, loc_pred = ssd_model(img)\n",
    "# print(conf_pred.shape, loc_pred.shape)\n",
    "trn_dataloader = DataLoader(train_dataset, config.batch_size, shuffle=False, collate_fn=detection_collate_fn)\n",
    "# lr_array, loss_array = lr_find(ssd_model, 1e-1, 1e-4, trn_dataloader, linear=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_state_dict = ssd_model.state_dict()\n",
    "trained_state_dict = torch.load(config.trained_path)\n",
    "trained2original = {'vgg':'base_net',\n",
    "                    'L2Norm':'l2_norm', 'extras': 'extra',\n",
    "                    'loc': 'loc_layers',\n",
    "                    'conf': 'conf_layers'\n",
    "                   }\n",
    "\n",
    "for trained_k in trained_state_dict.keys():\n",
    "    layer_name = trained_k.split('.')[0]\n",
    "    layer_num = trained_k.split('.')[1]\n",
    "    layer_other = '.'.join((trained_k.split('.')[2:]))\n",
    "    if layer_name == 'L2Norm':\n",
    "        original_key = 'l2_norm.weight'\n",
    "#         original_state_dict['l2_norm.weights'] = trained_state_dict[k]\n",
    "    elif layer_name == 'vgg' and int(layer_num) > 28:\n",
    "        num = str(int(layer_num) - 30)\n",
    "        original_key = '.'.join(['reduced_fc', num, layer_other])\n",
    "    else:\n",
    "        original_key = '.'.join([trained2original[layer_name], layer_num, layer_other])\n",
    "#     print('{:15s}:{:15s}'.format(trained_k, original_key))\n",
    "    original_state_dict[original_key] = trained_state_dict[trained_k]\n",
    "    \n",
    "ssd_model.load_state_dict(original_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "ssd_model = ssd_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1_path, img2_path = 'C:\\\\datasets\\\\pascal\\\\JPEGImages\\\\000012.jpg', 'C:\\\\datasets\\\\pascal\\\\JPEGImages\\\\000017.jpg'\n",
    "img1, img2 = cv2.imread(img1_path), cv2.imread(img2_path)\n",
    "img1, img2 = cv2.resize(img1, (300, 300)), cv2.resize(img2, (300, 300))\n",
    "img1, img2 = torch.FloatTensor(img1), torch.FloatTensor(img2)\n",
    "img1, img2 = img1.permute(2, 0, 1), img2.permute(2, 0, 1)\n",
    "\n",
    "imgs = torch.stack([img1, img2])\n",
    "imgs = imgs.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(228668.625, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "1 tensor(165391.703, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "2 tensor(34533.184, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "3 tensor(10194.548, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "4 tensor(6547.459, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "5 tensor(1612.559, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "conf, loc = ssd_model(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(57216460., device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-155.327, device='cuda:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-19960.617, device='cuda:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6785.321, device='cuda:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(ssd_model.l2_norm.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\py3\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 cls_loss: 9.103, loc_loss: 4.608, loss: 13.711\n",
      "1 cls_loss: 9.528, loc_loss: 3.767, loss: 13.295\n",
      "2 cls_loss: 9.228, loc_loss: 3.088, loss: 12.316\n",
      "3 cls_loss: 9.208, loc_loss: 7.935, loss: 17.142\n",
      "4 cls_loss: 8.811, loc_loss: 4.791, loss: 13.602\n",
      "5 cls_loss: 8.592, loc_loss: 4.327, loss: 12.92\n",
      "6 cls_loss: 8.326, loc_loss: 3.546, loss: 11.872\n",
      "7 cls_loss: 9.201, loc_loss: 9.054, loss: 18.254\n",
      "8 cls_loss: 8.948, loc_loss: 10.308, loss: 19.256\n",
      "9 cls_loss: 10.047, loc_loss: 5.539, loss: 15.586\n",
      "10 cls_loss: 10.427, loc_loss: 4.072, loss: 14.499\n",
      "11 cls_loss: 6.396, loc_loss: 5.328, loss: 11.724\n",
      "12 cls_loss: 9.155, loc_loss: 4.246, loss: 13.401\n",
      "13 cls_loss: 6.205, loc_loss: 7.109, loss: 13.314\n",
      "14 cls_loss: 9.29, loc_loss: 5.714, loss: 15.004\n",
      "15 cls_loss: 7.516, loc_loss: 4.717, loss: 12.234\n",
      "16 cls_loss: 8.497, loc_loss: 6.005, loss: 14.503\n",
      "17 cls_loss: 8.481, loc_loss: 6.423, loss: 14.904\n",
      "18 cls_loss: 10.271, loc_loss: 4.646, loss: 14.917\n",
      "19 cls_loss: 9.056, loc_loss: 6.009, loss: 15.066\n",
      "20 cls_loss: 9.275, loc_loss: 3.857, loss: 13.132\n",
      "21 cls_loss: 9.915, loc_loss: 4.587, loss: 14.502\n",
      "22 cls_loss: 9.53, loc_loss: 3.433, loss: 12.963\n",
      "23 cls_loss: 8.118, loc_loss: 7.572, loss: 15.69\n",
      "24 cls_loss: 9.159, loc_loss: 4.751, loss: 13.909\n",
      "25 cls_loss: 9.332, loc_loss: 3.913, loss: 13.245\n",
      "26 cls_loss: 7.673, loc_loss: 4.536, loss: 12.209\n",
      "27 cls_loss: 7.508, loc_loss: 5.693, loss: 13.201\n",
      "28 cls_loss: 8.691, loc_loss: 4.526, loss: 13.217\n",
      "29 cls_loss: 9.204, loc_loss: 5.371, loss: 14.576\n",
      "30 cls_loss: 10.133, loc_loss: 4.479, loss: 14.611\n",
      "31 cls_loss: 6.07, loc_loss: 6.962, loss: 13.032\n",
      "32 cls_loss: 8.398, loc_loss: 5.545, loss: 13.942\n",
      "33 cls_loss: 10.882, loc_loss: 4.051, loss: 14.933\n",
      "34 cls_loss: 11.06, loc_loss: 4.235, loss: 15.295\n",
      "35 cls_loss: 8.062, loc_loss: 8.296, loss: 16.358\n",
      "36 cls_loss: 8.971, loc_loss: 5.335, loss: 14.306\n",
      "37 cls_loss: 8.217, loc_loss: 7.279, loss: 15.496\n",
      "38 cls_loss: 8.052, loc_loss: 11.48, loss: 19.532\n",
      "39 cls_loss: 9.534, loc_loss: 3.788, loss: 13.322\n",
      "40 cls_loss: 8.677, loc_loss: 5.565, loss: 14.241\n",
      "41 cls_loss: 9.687, loc_loss: 4.782, loss: 14.469\n",
      "42 cls_loss: 8.892, loc_loss: 4.068, loss: 12.96\n",
      "43 cls_loss: 8.511, loc_loss: 7.109, loss: 15.62\n",
      "44 cls_loss: 11.424, loc_loss: 4.629, loss: 16.053\n",
      "45 cls_loss: 7.36, loc_loss: 5.448, loss: 12.808\n",
      "46 cls_loss: 7.388, loc_loss: 8.037, loss: 15.426\n",
      "47 cls_loss: 6.038, loc_loss: 11.145, loss: 17.183\n",
      "48 cls_loss: 10.282, loc_loss: 5.418, loss: 15.7\n",
      "49 cls_loss: 8.873, loc_loss: 5.354, loss: 14.226\n",
      "50 cls_loss: 7.513, loc_loss: 9.018, loss: 16.531\n",
      "51 cls_loss: 8.495, loc_loss: 5.835, loss: 14.33\n",
      "52 cls_loss: 9.364, loc_loss: 4.592, loss: 13.956\n",
      "53 cls_loss: 8.833, loc_loss: 4.99, loss: 13.823\n",
      "54 cls_loss: 5.828, loc_loss: 10.249, loss: 16.076\n",
      "55 cls_loss: 10.515, loc_loss: 9.06, loss: 19.575\n",
      "56 cls_loss: 8.794, loc_loss: 7.739, loss: 16.534\n",
      "57 cls_loss: 8.494, loc_loss: 5.666, loss: 14.16\n",
      "58 cls_loss: 8.355, loc_loss: 4.248, loss: 12.602\n",
      "59 cls_loss: 9.218, loc_loss: 4.774, loss: 13.992\n",
      "60 cls_loss: 8.603, loc_loss: 5.445, loss: 14.048\n",
      "61 cls_loss: 8.567, loc_loss: 9.047, loss: 17.614\n",
      "62 cls_loss: 9.093, loc_loss: 3.985, loss: 13.078\n",
      "63 cls_loss: 8.042, loc_loss: 6.585, loss: 14.627\n",
      "64 cls_loss: 8.337, loc_loss: 6.175, loss: 14.512\n",
      "65 cls_loss: 6.752, loc_loss: 13.461, loss: 20.213\n",
      "66 cls_loss: 9.152, loc_loss: 3.57, loss: 12.722\n",
      "67 cls_loss: 8.857, loc_loss: 5.532, loss: 14.389\n",
      "68 cls_loss: 10.237, loc_loss: 4.786, loss: 15.023\n",
      "69 cls_loss: 8.915, loc_loss: 5.95, loss: 14.865\n",
      "70 cls_loss: 10.425, loc_loss: 7.937, loss: 18.363\n",
      "71 cls_loss: 7.494, loc_loss: 7.643, loss: 15.137\n",
      "72 cls_loss: 9.81, loc_loss: 3.743, loss: 13.553\n",
      "73 cls_loss: 7.047, loc_loss: 11.412, loss: 18.459\n",
      "74 cls_loss: 8.25, loc_loss: 5.092, loss: 13.342\n",
      "75 cls_loss: 9.533, loc_loss: 7.871, loss: 17.404\n",
      "76 cls_loss: 7.458, loc_loss: 4.71, loss: 12.168\n",
      "77 cls_loss: 9.304, loc_loss: 5.107, loss: 14.411\n",
      "78 cls_loss: 6.563, loc_loss: 9.726, loss: 16.289\n",
      "79 cls_loss: 8.46, loc_loss: 5.809, loss: 14.269\n",
      "80 cls_loss: 9.638, loc_loss: 4.591, loss: 14.23\n",
      "81 cls_loss: 6.1, loc_loss: 3.854, loss: 9.954\n",
      "82 cls_loss: 7.949, loc_loss: 4.53, loss: 12.479\n",
      "83 cls_loss: 7.252, loc_loss: 9.044, loss: 16.296\n",
      "84 cls_loss: 9.596, loc_loss: 4.587, loss: 14.183\n",
      "85 cls_loss: 7.388, loc_loss: 8.014, loss: 15.402\n",
      "86 cls_loss: 8.905, loc_loss: 6.481, loss: 15.386\n",
      "87 cls_loss: 9.517, loc_loss: 5.279, loss: 14.795\n",
      "88 cls_loss: 7.288, loc_loss: 4.239, loss: 11.527\n",
      "89 cls_loss: 7.01, loc_loss: 6.271, loss: 13.281\n",
      "90 cls_loss: 9.703, loc_loss: 9.104, loss: 18.807\n",
      "91 cls_loss: 8.071, loc_loss: 10.951, loss: 19.022\n",
      "92 cls_loss: 10.061, loc_loss: 7.815, loss: 17.876\n",
      "93 cls_loss: 9.963, loc_loss: 4.78, loss: 14.742\n",
      "94 cls_loss: 7.229, loc_loss: 6.867, loss: 14.096\n",
      "95 cls_loss: 7.356, loc_loss: 15.922, loss: 23.278\n",
      "96 cls_loss: 8.216, loc_loss: 11.136, loss: 19.351\n",
      "97 cls_loss: 8.809, loc_loss: 5.701, loss: 14.509\n",
      "98 cls_loss: 6.627, loc_loss: 10.148, loss: 16.776\n",
      "99 cls_loss: 6.522, loc_loss: 4.95, loss: 11.473\n",
      "100 cls_loss: 10.497, loc_loss: 5.384, loss: 15.881\n",
      "101 cls_loss: 10.096, loc_loss: 6.292, loss: 16.388\n",
      "102 cls_loss: 11.023, loc_loss: 4.565, loss: 15.588\n",
      "103 cls_loss: 8.74, loc_loss: 4.116, loss: 12.856\n",
      "104 cls_loss: 10.514, loc_loss: 4.015, loss: 14.529\n",
      "105 cls_loss: 8.607, loc_loss: 8.368, loss: 16.975\n",
      "106 cls_loss: 8.608, loc_loss: 12.483, loss: 21.091\n",
      "107 cls_loss: 7.791, loc_loss: 4.588, loss: 12.379\n",
      "108 cls_loss: 9.013, loc_loss: 5.554, loss: 14.567\n",
      "109 cls_loss: 6.705, loc_loss: 10.07, loss: 16.775\n",
      "110 cls_loss: 9.095, loc_loss: 5.968, loss: 15.063\n",
      "111 cls_loss: 8.552, loc_loss: 3.536, loss: 12.089\n",
      "112 cls_loss: 10.343, loc_loss: 4.704, loss: 15.047\n",
      "113 cls_loss: 8.632, loc_loss: 3.369, loss: 12.001\n",
      "114 cls_loss: 10.035, loc_loss: 3.387, loss: 13.422\n",
      "115 cls_loss: 9.944, loc_loss: 5.795, loss: 15.74\n",
      "116 cls_loss: 10.614, loc_loss: 5.282, loss: 15.897\n",
      "117 cls_loss: 9.733, loc_loss: 10.549, loss: 20.282\n",
      "118 cls_loss: 7.694, loc_loss: 4.555, loss: 12.249\n",
      "119 cls_loss: 9.566, loc_loss: 6.15, loss: 15.716\n",
      "120 cls_loss: 8.196, loc_loss: 5.053, loss: 13.249\n",
      "121 cls_loss: 8.807, loc_loss: 7.934, loss: 16.741\n",
      "122 cls_loss: 6.123, loc_loss: 9.466, loss: 15.59\n",
      "123 cls_loss: 9.933, loc_loss: 6.659, loss: 16.592\n",
      "124 cls_loss: 10.357, loc_loss: 10.685, loss: 21.042\n",
      "125 cls_loss: 8.64, loc_loss: 8.545, loss: 17.185\n",
      "126 cls_loss: 10.383, loc_loss: 5.071, loss: 15.454\n",
      "127 cls_loss: 9.391, loc_loss: 5.274, loss: 14.665\n",
      "128 cls_loss: 7.496, loc_loss: 3.944, loss: 11.44\n",
      "129 cls_loss: 7.77, loc_loss: 5.86, loss: 13.63\n",
      "130 cls_loss: 9.259, loc_loss: 4.143, loss: 13.402\n",
      "131 cls_loss: 10.624, loc_loss: 5.285, loss: 15.909\n",
      "132 cls_loss: 7.352, loc_loss: 7.853, loss: 15.205\n",
      "133 cls_loss: 8.94, loc_loss: 4.784, loss: 13.724\n",
      "134 cls_loss: 11.225, loc_loss: 5.349, loss: 16.574\n",
      "135 cls_loss: 7.342, loc_loss: 5.724, loss: 13.067\n",
      "136 cls_loss: 11.084, loc_loss: 3.776, loss: 14.859\n",
      "137 cls_loss: 9.067, loc_loss: 4.366, loss: 13.433\n",
      "138 cls_loss: 8.047, loc_loss: 5.882, loss: 13.93\n",
      "139 cls_loss: 12.954, loc_loss: 4.868, loss: 17.822\n",
      "140 cls_loss: 9.544, loc_loss: 5.785, loss: 15.329\n",
      "141 cls_loss: 6.874, loc_loss: 12.517, loss: 19.391\n",
      "142 cls_loss: 6.69, loc_loss: 4.107, loss: 10.797\n",
      "143 cls_loss: 9.698, loc_loss: 7.018, loss: 16.715\n",
      "144 cls_loss: 10.671, loc_loss: 3.511, loss: 14.182\n",
      "145 cls_loss: 6.755, loc_loss: 10.188, loss: 16.943\n",
      "146 cls_loss: 7.651, loc_loss: 6.662, loss: 14.313\n",
      "147 cls_loss: 7.705, loc_loss: 4.65, loss: 12.355\n",
      "148 cls_loss: 8.416, loc_loss: 5.639, loss: 14.055\n",
      "149 cls_loss: 8.062, loc_loss: 4.865, loss: 12.926\n",
      "150 cls_loss: 8.431, loc_loss: 7.003, loss: 15.434\n",
      "151 cls_loss: 7.329, loc_loss: 6.137, loss: 13.466\n",
      "152 cls_loss: 10.448, loc_loss: 6.623, loss: 17.07\n",
      "153 cls_loss: 11.816, loc_loss: 4.985, loss: 16.801\n",
      "154 cls_loss: 8.139, loc_loss: 8.018, loss: 16.157\n",
      "155 cls_loss: 9.914, loc_loss: 4.039, loss: 13.954\n",
      "156 cls_loss: 9.436, loc_loss: 8.017, loss: 17.453\n",
      "157 cls_loss: 6.878, loc_loss: 9.672, loss: 16.55\n",
      "158 cls_loss: 9.907, loc_loss: 8.59, loss: 18.496\n",
      "159 cls_loss: 7.651, loc_loss: 4.635, loss: 12.286\n",
      "160 cls_loss: 10.02, loc_loss: 4.321, loss: 14.341\n",
      "161 cls_loss: 10.014, loc_loss: 8.322, loss: 18.336\n",
      "162 cls_loss: 9.073, loc_loss: 7.723, loss: 16.795\n",
      "163 cls_loss: 9.13, loc_loss: 7.767, loss: 16.897\n",
      "164 cls_loss: 7.449, loc_loss: 6.123, loss: 13.572\n",
      "165 cls_loss: 10.022, loc_loss: 6.564, loss: 16.586\n",
      "166 cls_loss: 10.75, loc_loss: 3.566, loss: 14.316\n",
      "167 cls_loss: 8.499, loc_loss: 4.308, loss: 12.807\n",
      "168 cls_loss: 11.315, loc_loss: 4.589, loss: 15.904\n",
      "169 cls_loss: 7.384, loc_loss: 5.898, loss: 13.282\n",
      "170 cls_loss: 10.136, loc_loss: 4.116, loss: 14.252\n",
      "171 cls_loss: 10.685, loc_loss: 7.718, loss: 18.403\n",
      "172 cls_loss: 8.637, loc_loss: 6.812, loss: 15.449\n",
      "173 cls_loss: 9.156, loc_loss: 9.499, loss: 18.655\n",
      "174 cls_loss: 8.159, loc_loss: 7.062, loss: 15.221\n",
      "175 cls_loss: 9.521, loc_loss: 4.979, loss: 14.499\n",
      "176 cls_loss: 7.254, loc_loss: 7.743, loss: 14.996\n",
      "177 cls_loss: 8.947, loc_loss: 5.355, loss: 14.302\n",
      "178 cls_loss: 9.369, loc_loss: 5.199, loss: 14.568\n",
      "179 cls_loss: 9.317, loc_loss: 7.388, loss: 16.705\n",
      "180 cls_loss: 9.986, loc_loss: 3.201, loss: 13.188\n",
      "181 cls_loss: 9.339, loc_loss: 4.225, loss: 13.564\n",
      "182 cls_loss: 9.918, loc_loss: 4.43, loss: 14.348\n",
      "183 cls_loss: 9.726, loc_loss: 4.668, loss: 14.393\n",
      "184 cls_loss: 7.979, loc_loss: 6.106, loss: 14.085\n",
      "185 cls_loss: 8.496, loc_loss: 4.692, loss: 13.188\n",
      "186 cls_loss: 8.152, loc_loss: 5.085, loss: 13.238\n",
      "187 cls_loss: 8.457, loc_loss: 9.767, loss: 18.224\n",
      "188 cls_loss: 8.64, loc_loss: 7.74, loss: 16.38\n",
      "189 cls_loss: 9.332, loc_loss: 8.915, loss: 18.246\n",
      "190 cls_loss: 9.024, loc_loss: 10.914, loss: 19.938\n",
      "191 cls_loss: 8.65, loc_loss: 4.13, loss: 12.779\n",
      "192 cls_loss: 9.604, loc_loss: 7.212, loss: 16.815\n",
      "193 cls_loss: 7.728, loc_loss: 5.531, loss: 13.259\n",
      "194 cls_loss: 7.973, loc_loss: 5.708, loss: 13.681\n",
      "195 cls_loss: 9.013, loc_loss: 3.328, loss: 12.341\n",
      "196 cls_loss: 7.83, loc_loss: 6.349, loss: 14.179\n",
      "197 cls_loss: 9.349, loc_loss: 6.078, loss: 15.427\n",
      "198 cls_loss: 7.032, loc_loss: 11.383, loss: 18.415\n",
      "199 cls_loss: 8.396, loc_loss: 10.795, loss: 19.19\n",
      "200 cls_loss: 8.101, loc_loss: 4.829, loss: 12.931\n",
      "201 cls_loss: 10.065, loc_loss: 6.935, loss: 17.0\n",
      "202 cls_loss: 8.589, loc_loss: 6.967, loss: 15.557\n",
      "203 cls_loss: 7.051, loc_loss: 3.914, loss: 10.965\n",
      "204 cls_loss: 7.542, loc_loss: 5.271, loss: 12.813\n",
      "205 cls_loss: 7.146, loc_loss: 10.429, loss: 17.574\n",
      "206 cls_loss: 10.077, loc_loss: 6.034, loss: 16.111\n",
      "207 cls_loss: 9.791, loc_loss: 3.522, loss: 13.312\n",
      "208 cls_loss: 8.794, loc_loss: 8.476, loss: 17.271\n",
      "209 cls_loss: 9.096, loc_loss: 9.203, loss: 18.299\n",
      "210 cls_loss: 9.887, loc_loss: 5.486, loss: 15.373\n",
      "211 cls_loss: 9.848, loc_loss: 3.603, loss: 13.451\n",
      "212 cls_loss: 8.918, loc_loss: 5.643, loss: 14.561\n",
      "213 cls_loss: 9.075, loc_loss: 6.58, loss: 15.655\n",
      "214 cls_loss: 8.889, loc_loss: 9.553, loss: 18.442\n",
      "215 cls_loss: 7.989, loc_loss: 14.769, loss: 22.757\n",
      "216 cls_loss: 8.236, loc_loss: 4.096, loss: 12.332\n",
      "217 cls_loss: 9.095, loc_loss: 5.486, loss: 14.581\n",
      "218 cls_loss: 9.857, loc_loss: 4.064, loss: 13.921\n",
      "219 cls_loss: 10.689, loc_loss: 3.07, loss: 13.759\n",
      "220 cls_loss: 9.117, loc_loss: 5.859, loss: 14.976\n",
      "221 cls_loss: 8.395, loc_loss: 3.983, loss: 12.378\n",
      "222 cls_loss: 8.145, loc_loss: 8.422, loss: 16.568\n",
      "223 cls_loss: 9.64, loc_loss: 3.729, loss: 13.368\n",
      "224 cls_loss: 8.715, loc_loss: 5.04, loss: 13.755\n",
      "225 cls_loss: 8.464, loc_loss: 4.674, loss: 13.137\n",
      "226 cls_loss: 6.693, loc_loss: 7.59, loss: 14.283\n",
      "227 cls_loss: 9.638, loc_loss: 5.004, loss: 14.643\n",
      "228 cls_loss: 8.001, loc_loss: 5.021, loss: 13.022\n",
      "229 cls_loss: 9.825, loc_loss: 4.698, loss: 14.523\n",
      "230 cls_loss: 9.663, loc_loss: 7.23, loss: 16.893\n",
      "231 cls_loss: 9.079, loc_loss: 8.222, loss: 17.301\n",
      "232 cls_loss: 8.834, loc_loss: 5.85, loss: 14.683\n",
      "233 cls_loss: 7.631, loc_loss: 7.153, loss: 14.784\n",
      "234 cls_loss: 9.869, loc_loss: 5.138, loss: 15.006\n",
      "235 cls_loss: 7.052, loc_loss: 4.314, loss: 11.365\n",
      "236 cls_loss: 8.924, loc_loss: 7.988, loss: 16.912\n",
      "237 cls_loss: 8.838, loc_loss: 3.863, loss: 12.701\n",
      "238 cls_loss: 9.578, loc_loss: 3.922, loss: 13.5\n",
      "239 cls_loss: 8.814, loc_loss: 5.07, loss: 13.884\n",
      "240 cls_loss: 7.713, loc_loss: 5.201, loss: 12.914\n",
      "241 cls_loss: 7.876, loc_loss: 5.298, loss: 13.174\n",
      "242 cls_loss: 7.875, loc_loss: 5.582, loss: 13.457\n",
      "243 cls_loss: 8.732, loc_loss: 5.94, loss: 14.672\n",
      "244 cls_loss: 9.043, loc_loss: 8.697, loss: 17.74\n",
      "245 cls_loss: 8.191, loc_loss: 9.781, loss: 17.972\n",
      "246 cls_loss: 8.61, loc_loss: 9.606, loss: 18.216\n",
      "247 cls_loss: 8.988, loc_loss: 3.855, loss: 12.842\n",
      "248 cls_loss: 8.178, loc_loss: 13.043, loss: 21.221\n",
      "249 cls_loss: 7.266, loc_loss: 10.212, loss: 17.477\n",
      "250 cls_loss: 8.576, loc_loss: 5.077, loss: 13.653\n",
      "251 cls_loss: 9.788, loc_loss: 4.167, loss: 13.955\n",
      "252 cls_loss: 9.422, loc_loss: 3.591, loss: 13.013\n",
      "253 cls_loss: 6.418, loc_loss: 9.709, loss: 16.127\n",
      "254 cls_loss: 8.729, loc_loss: 9.841, loss: 18.57\n",
      "255 cls_loss: 10.09, loc_loss: 6.3, loss: 16.391\n",
      "256 cls_loss: 8.297, loc_loss: 8.055, loss: 16.352\n",
      "257 cls_loss: 7.657, loc_loss: 6.649, loss: 14.306\n",
      "258 cls_loss: 9.195, loc_loss: 4.728, loss: 13.922\n",
      "259 cls_loss: 8.015, loc_loss: 7.754, loss: 15.769\n",
      "260 cls_loss: 8.177, loc_loss: 3.416, loss: 11.593\n",
      "261 cls_loss: 12.068, loc_loss: 4.28, loss: 16.348\n",
      "262 cls_loss: 9.05, loc_loss: 5.581, loss: 14.631\n",
      "263 cls_loss: 6.853, loc_loss: 6.142, loss: 12.996\n",
      "264 cls_loss: 8.617, loc_loss: 6.053, loss: 14.67\n",
      "265 cls_loss: 8.004, loc_loss: 7.74, loss: 15.744\n",
      "266 cls_loss: 8.107, loc_loss: 6.122, loss: 14.229\n",
      "267 cls_loss: 8.052, loc_loss: 5.858, loss: 13.91\n",
      "268 cls_loss: 8.738, loc_loss: 6.321, loss: 15.058\n",
      "269 cls_loss: 8.057, loc_loss: 4.576, loss: 12.633\n",
      "270 cls_loss: 8.218, loc_loss: 10.606, loss: 18.825\n",
      "271 cls_loss: 8.322, loc_loss: 6.037, loss: 14.36\n",
      "272 cls_loss: 7.335, loc_loss: 10.521, loss: 17.856\n",
      "273 cls_loss: 8.425, loc_loss: 4.553, loss: 12.978\n",
      "274 cls_loss: 9.597, loc_loss: 8.164, loss: 17.761\n",
      "275 cls_loss: 8.922, loc_loss: 3.92, loss: 12.842\n",
      "276 cls_loss: 8.122, loc_loss: 5.123, loss: 13.245\n",
      "277 cls_loss: 7.064, loc_loss: 8.148, loss: 15.211\n",
      "278 cls_loss: 10.386, loc_loss: 4.68, loss: 15.066\n",
      "279 cls_loss: 7.042, loc_loss: 16.27, loss: 23.312\n",
      "280 cls_loss: 8.477, loc_loss: 15.846, loss: 24.322\n",
      "281 cls_loss: 7.387, loc_loss: 6.321, loss: 13.708\n",
      "282 cls_loss: 9.163, loc_loss: 7.437, loss: 16.599\n",
      "283 cls_loss: 9.131, loc_loss: 4.93, loss: 14.061\n",
      "284 cls_loss: 8.071, loc_loss: 8.217, loss: 16.288\n",
      "285 cls_loss: 9.494, loc_loss: 3.764, loss: 13.258\n",
      "286 cls_loss: 10.091, loc_loss: 4.829, loss: 14.921\n",
      "287 cls_loss: 8.515, loc_loss: 6.598, loss: 15.113\n",
      "288 cls_loss: 10.864, loc_loss: 5.269, loss: 16.133\n",
      "289 cls_loss: 5.145, loc_loss: 13.679, loss: 18.824\n",
      "290 cls_loss: 9.751, loc_loss: 5.095, loss: 14.846\n",
      "291 cls_loss: 10.152, loc_loss: 8.14, loss: 18.292\n",
      "292 cls_loss: 10.085, loc_loss: 4.817, loss: 14.902\n",
      "293 cls_loss: 9.02, loc_loss: 5.33, loss: 14.35\n",
      "294 cls_loss: 6.823, loc_loss: 5.836, loss: 12.659\n",
      "295 cls_loss: 7.705, loc_loss: 5.363, loss: 13.069\n",
      "296 cls_loss: 9.16, loc_loss: 3.561, loss: 12.721\n",
      "297 cls_loss: 7.716, loc_loss: 6.322, loss: 14.037\n",
      "298 cls_loss: 8.108, loc_loss: 5.873, loss: 13.981\n",
      "299 cls_loss: 7.345, loc_loss: 5.201, loss: 12.546\n",
      "300 cls_loss: 7.876, loc_loss: 4.553, loss: 12.429\n",
      "301 cls_loss: 9.23, loc_loss: 6.208, loss: 15.438\n",
      "302 cls_loss: 8.916, loc_loss: 4.123, loss: 13.038\n",
      "303 cls_loss: 9.062, loc_loss: 4.501, loss: 13.563\n",
      "304 cls_loss: 10.036, loc_loss: 4.11, loss: 14.146\n",
      "305 cls_loss: 7.535, loc_loss: 3.868, loss: 11.403\n",
      "306 cls_loss: 8.213, loc_loss: 9.785, loss: 17.998\n",
      "307 cls_loss: 6.931, loc_loss: 6.014, loss: 12.945\n",
      "308 cls_loss: 8.746, loc_loss: 13.112, loss: 21.858\n",
      "309 cls_loss: 8.937, loc_loss: 5.525, loss: 14.461\n",
      "310 cls_loss: 9.191, loc_loss: 4.134, loss: 13.325\n",
      "311 cls_loss: 9.983, loc_loss: 3.849, loss: 13.832\n",
      "312 cls_loss: 9.309, loc_loss: 3.49, loss: 12.8\n",
      "313 cls_loss: 8.972, loc_loss: 5.983, loss: 14.955\n",
      "314 cls_loss: 6.842, loc_loss: 9.738, loss: 16.58\n",
      "315 cls_loss: 9.417, loc_loss: 3.949, loss: 13.366\n",
      "316 cls_loss: 8.683, loc_loss: 11.034, loss: 19.717\n",
      "317 cls_loss: 7.712, loc_loss: 5.538, loss: 13.25\n",
      "318 cls_loss: 8.226, loc_loss: 9.832, loss: 18.057\n",
      "319 cls_loss: 9.357, loc_loss: 7.831, loss: 17.188\n",
      "320 cls_loss: 8.313, loc_loss: 5.547, loss: 13.86\n",
      "321 cls_loss: 9.02, loc_loss: 7.367, loss: 16.387\n",
      "322 cls_loss: 7.641, loc_loss: 7.542, loss: 15.182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323 cls_loss: 7.187, loc_loss: 5.416, loss: 12.602\n",
      "324 cls_loss: 9.172, loc_loss: 5.714, loss: 14.886\n",
      "325 cls_loss: 8.605, loc_loss: 7.542, loss: 16.147\n",
      "326 cls_loss: 10.065, loc_loss: 6.727, loss: 16.792\n",
      "327 cls_loss: 8.126, loc_loss: 5.625, loss: 13.752\n",
      "328 cls_loss: 8.215, loc_loss: 7.198, loss: 15.412\n",
      "329 cls_loss: 6.03, loc_loss: 4.316, loss: 10.346\n",
      "330 cls_loss: 10.816, loc_loss: 7.623, loss: 18.44\n",
      "331 cls_loss: 10.078, loc_loss: 6.571, loss: 16.649\n",
      "332 cls_loss: 10.149, loc_loss: 4.471, loss: 14.62\n",
      "333 cls_loss: 9.258, loc_loss: 9.792, loss: 19.051\n",
      "334 cls_loss: 11.693, loc_loss: 3.753, loss: 15.446\n",
      "335 cls_loss: 5.477, loc_loss: 12.576, loss: 18.053\n",
      "336 cls_loss: 7.871, loc_loss: 4.874, loss: 12.745\n",
      "337 cls_loss: 8.877, loc_loss: 4.493, loss: 13.37\n",
      "338 cls_loss: 7.909, loc_loss: 5.69, loss: 13.599\n",
      "339 cls_loss: 8.332, loc_loss: 7.047, loss: 15.379\n",
      "340 cls_loss: 8.8, loc_loss: 8.016, loss: 16.816\n",
      "341 cls_loss: 9.133, loc_loss: 7.441, loss: 16.574\n",
      "342 cls_loss: 9.842, loc_loss: 3.896, loss: 13.738\n",
      "343 cls_loss: 7.427, loc_loss: 6.177, loss: 13.605\n",
      "344 cls_loss: 10.248, loc_loss: 3.96, loss: 14.208\n",
      "345 cls_loss: 10.225, loc_loss: 4.228, loss: 14.452\n",
      "346 cls_loss: 8.732, loc_loss: 7.725, loss: 16.457\n",
      "347 cls_loss: 8.055, loc_loss: 4.438, loss: 12.493\n",
      "348 cls_loss: 8.754, loc_loss: 8.292, loss: 17.046\n",
      "349 cls_loss: 9.016, loc_loss: 5.85, loss: 14.867\n",
      "350 cls_loss: 9.59, loc_loss: 4.659, loss: 14.249\n",
      "351 cls_loss: 7.766, loc_loss: 4.049, loss: 11.815\n",
      "352 cls_loss: 9.428, loc_loss: 4.531, loss: 13.959\n",
      "353 cls_loss: 10.578, loc_loss: 3.576, loss: 14.153\n",
      "354 cls_loss: 8.571, loc_loss: 6.767, loss: 15.338\n",
      "355 cls_loss: 9.967, loc_loss: 5.412, loss: 15.379\n",
      "356 cls_loss: 9.182, loc_loss: 5.732, loss: 14.914\n",
      "357 cls_loss: 9.084, loc_loss: 4.587, loss: 13.671\n",
      "358 cls_loss: 9.268, loc_loss: 9.231, loss: 18.499\n",
      "359 cls_loss: 8.994, loc_loss: 7.317, loss: 16.311\n",
      "360 cls_loss: 8.713, loc_loss: 4.751, loss: 13.464\n",
      "361 cls_loss: 8.984, loc_loss: 6.645, loss: 15.629\n",
      "362 cls_loss: 6.389, loc_loss: 10.607, loss: 16.997\n",
      "363 cls_loss: 9.367, loc_loss: 6.908, loss: 16.276\n",
      "364 cls_loss: 8.329, loc_loss: 10.403, loss: 18.732\n",
      "365 cls_loss: 8.817, loc_loss: 4.124, loss: 12.941\n",
      "366 cls_loss: 8.562, loc_loss: 5.606, loss: 14.168\n",
      "367 cls_loss: 7.959, loc_loss: 6.672, loss: 14.631\n",
      "368 cls_loss: 8.798, loc_loss: 6.5, loss: 15.298\n",
      "369 cls_loss: 9.419, loc_loss: 6.933, loss: 16.351\n",
      "370 cls_loss: 4.418, loc_loss: 10.847, loss: 15.265\n",
      "371 cls_loss: 10.151, loc_loss: 7.952, loss: 18.103\n",
      "372 cls_loss: 8.51, loc_loss: 7.365, loss: 15.875\n",
      "373 cls_loss: 7.663, loc_loss: 6.384, loss: 14.047\n",
      "374 cls_loss: 8.382, loc_loss: 5.175, loss: 13.558\n",
      "375 cls_loss: 7.112, loc_loss: 10.108, loss: 17.22\n",
      "376 cls_loss: 8.888, loc_loss: 4.372, loss: 13.26\n",
      "377 cls_loss: 9.777, loc_loss: 8.015, loss: 17.793\n",
      "378 cls_loss: 7.12, loc_loss: 11.689, loss: 18.808\n",
      "379 cls_loss: 7.732, loc_loss: 6.734, loss: 14.466\n",
      "380 cls_loss: 8.323, loc_loss: 7.648, loss: 15.971\n",
      "381 cls_loss: 7.234, loc_loss: 9.071, loss: 16.305\n",
      "382 cls_loss: 9.656, loc_loss: 6.142, loss: 15.798\n",
      "383 cls_loss: 7.055, loc_loss: 8.917, loss: 15.973\n",
      "384 cls_loss: 10.017, loc_loss: 4.284, loss: 14.301\n",
      "385 cls_loss: 9.413, loc_loss: 6.631, loss: 16.044\n",
      "386 cls_loss: 7.962, loc_loss: 11.038, loss: 19.0\n",
      "387 cls_loss: 7.792, loc_loss: 7.942, loss: 15.734\n",
      "388 cls_loss: 8.021, loc_loss: 6.252, loss: 14.274\n",
      "389 cls_loss: 9.341, loc_loss: 5.357, loss: 14.697\n",
      "390 cls_loss: 8.3, loc_loss: 5.421, loss: 13.721\n",
      "391 cls_loss: 9.507, loc_loss: 9.577, loss: 19.084\n",
      "392 cls_loss: 8.754, loc_loss: 4.395, loss: 13.149\n",
      "393 cls_loss: 8.164, loc_loss: 5.477, loss: 13.642\n",
      "394 cls_loss: 7.391, loc_loss: 3.899, loss: 11.289\n",
      "395 cls_loss: 6.406, loc_loss: 7.842, loss: 14.248\n",
      "396 cls_loss: 8.499, loc_loss: 6.167, loss: 14.666\n",
      "397 cls_loss: 9.259, loc_loss: 5.961, loss: 15.22\n",
      "398 cls_loss: 7.423, loc_loss: 14.722, loss: 22.146\n",
      "399 cls_loss: 7.455, loc_loss: 9.174, loss: 16.63\n",
      "400 cls_loss: 9.379, loc_loss: 5.591, loss: 14.97\n",
      "401 cls_loss: 6.89, loc_loss: 4.804, loss: 11.694\n",
      "402 cls_loss: 6.88, loc_loss: 4.63, loss: 11.511\n",
      "403 cls_loss: 7.116, loc_loss: 4.263, loss: 11.379\n",
      "404 cls_loss: 8.072, loc_loss: 5.548, loss: 13.62\n",
      "405 cls_loss: 9.186, loc_loss: 4.262, loss: 13.448\n",
      "406 cls_loss: 9.9, loc_loss: 5.366, loss: 15.265\n",
      "407 cls_loss: 8.365, loc_loss: 6.904, loss: 15.269\n",
      "408 cls_loss: 11.309, loc_loss: 3.591, loss: 14.9\n",
      "409 cls_loss: 6.89, loc_loss: 8.396, loss: 15.286\n",
      "410 cls_loss: 9.277, loc_loss: 3.735, loss: 13.012\n",
      "411 cls_loss: 8.211, loc_loss: 4.233, loss: 12.444\n",
      "412 cls_loss: 7.329, loc_loss: 10.174, loss: 17.502\n",
      "413 cls_loss: 10.373, loc_loss: 5.231, loss: 15.604\n",
      "414 cls_loss: 9.991, loc_loss: 4.149, loss: 14.141\n",
      "415 cls_loss: 8.18, loc_loss: 5.016, loss: 13.196\n",
      "416 cls_loss: 7.122, loc_loss: 9.827, loss: 16.949\n",
      "417 cls_loss: 7.288, loc_loss: 4.397, loss: 11.685\n",
      "418 cls_loss: 8.586, loc_loss: 5.734, loss: 14.32\n",
      "419 cls_loss: 9.394, loc_loss: 5.489, loss: 14.883\n",
      "420 cls_loss: 9.79, loc_loss: 5.851, loss: 15.641\n",
      "421 cls_loss: 8.44, loc_loss: 4.262, loss: 12.702\n",
      "422 cls_loss: 5.146, loc_loss: 6.124, loss: 11.27\n",
      "423 cls_loss: 9.266, loc_loss: 4.011, loss: 13.277\n",
      "424 cls_loss: 8.207, loc_loss: 3.921, loss: 12.129\n",
      "425 cls_loss: 8.406, loc_loss: 6.868, loss: 15.274\n",
      "426 cls_loss: 5.728, loc_loss: 10.055, loss: 15.782\n",
      "427 cls_loss: 9.229, loc_loss: 9.708, loss: 18.936\n",
      "428 cls_loss: 8.405, loc_loss: 11.882, loss: 20.287\n",
      "429 cls_loss: 8.896, loc_loss: 4.843, loss: 13.739\n",
      "430 cls_loss: 8.737, loc_loss: 3.137, loss: 11.874\n",
      "431 cls_loss: 7.85, loc_loss: 3.835, loss: 11.685\n",
      "432 cls_loss: 10.54, loc_loss: 4.347, loss: 14.888\n",
      "433 cls_loss: 9.07, loc_loss: 5.061, loss: 14.131\n",
      "434 cls_loss: 9.844, loc_loss: 4.073, loss: 13.917\n",
      "435 cls_loss: 9.292, loc_loss: 7.828, loss: 17.12\n",
      "436 cls_loss: 6.818, loc_loss: 8.449, loss: 15.267\n",
      "437 cls_loss: 9.02, loc_loss: 4.958, loss: 13.977\n",
      "438 cls_loss: 8.393, loc_loss: 7.982, loss: 16.375\n",
      "439 cls_loss: 7.836, loc_loss: 4.467, loss: 12.304\n",
      "440 cls_loss: 8.773, loc_loss: 5.139, loss: 13.912\n",
      "441 cls_loss: 7.992, loc_loss: 3.857, loss: 11.849\n",
      "442 cls_loss: 7.675, loc_loss: 7.972, loss: 15.647\n",
      "443 cls_loss: 9.171, loc_loss: 4.696, loss: 13.867\n",
      "444 cls_loss: 11.65, loc_loss: 4.523, loss: 16.173\n",
      "445 cls_loss: 8.529, loc_loss: 3.529, loss: 12.058\n",
      "446 cls_loss: 10.598, loc_loss: 4.226, loss: 14.824\n",
      "447 cls_loss: 9.21, loc_loss: 6.482, loss: 15.692\n",
      "448 cls_loss: 8.582, loc_loss: 3.891, loss: 12.473\n",
      "449 cls_loss: 9.2, loc_loss: 3.743, loss: 12.943\n",
      "450 cls_loss: 7.329, loc_loss: 7.08, loss: 14.409\n",
      "451 cls_loss: 8.29, loc_loss: 10.548, loss: 18.838\n",
      "452 cls_loss: 7.32, loc_loss: 12.232, loss: 19.553\n",
      "453 cls_loss: 8.994, loc_loss: 6.543, loss: 15.537\n",
      "454 cls_loss: 8.108, loc_loss: 7.932, loss: 16.04\n",
      "455 cls_loss: 9.231, loc_loss: 4.574, loss: 13.805\n",
      "456 cls_loss: 6.576, loc_loss: 7.593, loss: 14.169\n",
      "457 cls_loss: 10.019, loc_loss: 4.813, loss: 14.831\n",
      "458 cls_loss: 9.118, loc_loss: 5.377, loss: 14.495\n",
      "459 cls_loss: 8.873, loc_loss: 5.613, loss: 14.486\n",
      "460 cls_loss: 9.533, loc_loss: 4.658, loss: 14.191\n",
      "461 cls_loss: 7.899, loc_loss: 8.83, loss: 16.728\n",
      "462 cls_loss: 8.823, loc_loss: 6.133, loss: 14.955\n",
      "463 cls_loss: 8.373, loc_loss: 10.532, loss: 18.905\n",
      "464 cls_loss: 8.382, loc_loss: 5.598, loss: 13.98\n",
      "465 cls_loss: 6.73, loc_loss: 5.007, loss: 11.737\n",
      "466 cls_loss: 9.204, loc_loss: 4.626, loss: 13.83\n",
      "467 cls_loss: 9.376, loc_loss: 5.095, loss: 14.471\n",
      "468 cls_loss: 7.788, loc_loss: 7.189, loss: 14.977\n",
      "469 cls_loss: 9.229, loc_loss: 4.575, loss: 13.804\n",
      "470 cls_loss: 9.348, loc_loss: 6.956, loss: 16.304\n",
      "471 cls_loss: 7.656, loc_loss: 3.975, loss: 11.631\n",
      "472 cls_loss: 10.283, loc_loss: 5.934, loss: 16.217\n",
      "473 cls_loss: 8.543, loc_loss: 4.969, loss: 13.512\n",
      "474 cls_loss: 7.304, loc_loss: 8.169, loss: 15.474\n",
      "475 cls_loss: 8.996, loc_loss: 4.689, loss: 13.685\n",
      "476 cls_loss: 10.122, loc_loss: 6.25, loss: 16.372\n",
      "477 cls_loss: 9.202, loc_loss: 4.654, loss: 13.855\n",
      "478 cls_loss: 8.662, loc_loss: 8.445, loss: 17.107\n",
      "479 cls_loss: 9.768, loc_loss: 3.96, loss: 13.727\n",
      "480 cls_loss: 8.77, loc_loss: 7.758, loss: 16.528\n",
      "481 cls_loss: 8.203, loc_loss: 10.98, loss: 19.183\n",
      "482 cls_loss: 7.514, loc_loss: 7.115, loss: 14.629\n",
      "483 cls_loss: 11.566, loc_loss: 4.24, loss: 15.806\n",
      "484 cls_loss: 7.841, loc_loss: 7.05, loss: 14.891\n",
      "485 cls_loss: 10.002, loc_loss: 4.816, loss: 14.817\n",
      "486 cls_loss: 8.681, loc_loss: 4.285, loss: 12.967\n",
      "487 cls_loss: 9.352, loc_loss: 11.39, loss: 20.743\n",
      "488 cls_loss: 9.84, loc_loss: 8.321, loss: 18.16\n",
      "489 cls_loss: 7.786, loc_loss: 4.222, loss: 12.008\n",
      "490 cls_loss: 7.692, loc_loss: 7.242, loss: 14.934\n",
      "491 cls_loss: 11.738, loc_loss: 4.061, loss: 15.799\n",
      "492 cls_loss: 9.799, loc_loss: 4.76, loss: 14.56\n",
      "493 cls_loss: 7.671, loc_loss: 6.862, loss: 14.533\n",
      "494 cls_loss: 8.242, loc_loss: 5.536, loss: 13.779\n",
      "495 cls_loss: 8.984, loc_loss: 5.364, loss: 14.348\n",
      "496 cls_loss: 10.685, loc_loss: 6.372, loss: 17.057\n",
      "497 cls_loss: 8.566, loc_loss: 9.475, loss: 18.041\n",
      "498 cls_loss: 8.371, loc_loss: 8.28, loss: 16.651\n",
      "499 cls_loss: 9.912, loc_loss: 7.528, loss: 17.44\n",
      "500 cls_loss: 9.87, loc_loss: 4.734, loss: 14.604\n",
      "501 cls_loss: 9.885, loc_loss: 4.204, loss: 14.09\n",
      "502 cls_loss: 8.965, loc_loss: 4.878, loss: 13.843\n",
      "503 cls_loss: 8.63, loc_loss: 3.468, loss: 12.099\n",
      "504 cls_loss: 9.098, loc_loss: 4.797, loss: 13.895\n",
      "505 cls_loss: 7.684, loc_loss: 5.607, loss: 13.292\n",
      "506 cls_loss: 7.02, loc_loss: 8.945, loss: 15.965\n",
      "507 cls_loss: 10.286, loc_loss: 7.749, loss: 18.035\n",
      "508 cls_loss: 5.317, loc_loss: 13.95, loss: 19.267\n",
      "509 cls_loss: 8.266, loc_loss: 5.216, loss: 13.482\n",
      "510 cls_loss: 7.983, loc_loss: 8.34, loss: 16.322\n",
      "511 cls_loss: 7.527, loc_loss: 6.305, loss: 13.832\n",
      "512 cls_loss: 7.055, loc_loss: 7.522, loss: 14.577\n",
      "513 cls_loss: 9.919, loc_loss: 6.397, loss: 16.316\n",
      "514 cls_loss: 6.883, loc_loss: 14.649, loss: 21.533\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-45965b41f282>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbboxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mcls_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls_preds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc_preds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0miou\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_iou\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprior_box\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mpos_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbbox_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miou\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprior_box\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\1-usc\\SSD_PyTorch\\SSDloss.py\u001b[0m in \u001b[0;36mget_iou\u001b[1;34m(bbox, prior_box)\u001b[0m\n\u001b[0;32m     90\u001b[0m                             \u001b[1;33m(\u001b[0m\u001b[0mprior_box\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mprior_box\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m                             \u001b[1;33m(\u001b[0m\u001b[0mprior_box\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mprior_box\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m                             (prior_box[:, 0] + prior_box[:, 3] / 2).unsqueeze(1)], dim=1)\n\u001b[0m\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[0mA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "prior_box = get_prior_box()\n",
    "loss_array = []\n",
    "\n",
    "for i, batch in enumerate(trn_dataloader):\n",
    "\n",
    "    imgs, bboxes, labels = batch\n",
    "    imgs = imgs.to(device)\n",
    "    cls_preds, loc_preds = ssd_model(imgs)\n",
    "\n",
    "#     model.zero_grad()\n",
    "\n",
    "    total_loss = 0\n",
    "    total_loc_loss, total_cls_loss = 0, 0\n",
    "\n",
    "    for idx in range(imgs.shape[0]):\n",
    "\n",
    "        img, bbox, label = imgs[idx], bboxes[idx], labels[idx]\n",
    "        cls_pred, loc_pred = cls_preds[idx], loc_preds[idx]\n",
    "        iou = get_iou(bbox, prior_box)\n",
    "\n",
    "        pos_mask, cls_target, bbox_target = get_target(iou, prior_box, img, bbox, label)\n",
    "        pos_mask, cls_target, bbox_target = pos_mask.to(device), cls_target.to(device), bbox_target.to(device)\n",
    "\n",
    "        loss_loc, loss_cls = loss(cls_pred, loc_pred, pos_mask, cls_target, bbox_target)\n",
    "        total_loc_loss += loss_loc; total_cls_loss += loss_cls\n",
    "\n",
    "        total_loss += (loss_cls + loss_loc)\n",
    "\n",
    "    total_loss /= float(imgs.shape[0])\n",
    "    total_cls_loss /= float(imgs.shape[0])\n",
    "    total_loc_loss /= float(imgs.shape[0])\n",
    "\n",
    "#     total_loss.backward()\n",
    "\n",
    "#     optimizer.step()\n",
    "    cls_loss = round(float(total_cls_loss), 3)\n",
    "    loc_loss = round(float(total_loc_loss), 3)\n",
    "    t_loss = round(float(total_loss), 3)\n",
    "\n",
    "    print(i, 'cls_loss: {}, loc_loss: {}, loss: {}'.format(cls_loss, loc_loss, t_loss))\n",
    "    loss_array.append(t_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3], device='cuda:0', dtype=torch.int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_target[pos_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([78, 21])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(cls_pred[pos_mask].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 5, 5])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = nn.Conv2d(1, 1, kernel_size=(3, 3), stride=1, padding=0)\n",
    "l.weight = nn.Parameter(torch.ones(1, 1, 3, 3).float())\n",
    "x = torch.zeros(25)\n",
    "for i in range(25):\n",
    "    x[i] = i\n",
    "x = x.reshape(1, 1, 5, 5)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.]]]], requires_grad=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "l.bias = nn.Parameter(torch.FloatTensor([0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  1.,  2.,  3.,  4.],\n",
       "          [ 5.,  6.,  7.,  8.,  9.],\n",
       "          [10., 11., 12., 13., 14.],\n",
       "          [15., 16., 17., 18., 19.],\n",
       "          [20., 21., 22., 23., 24.]]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 54.,  63.,  72.],\n",
       "          [ 99., 108., 117.],\n",
       "          [144., 153., 162.]]]], grad_fn=<ThnnConv2DBackward>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1_path, img2_path = 'C:\\\\datasets\\\\pascal\\\\JPEGImages\\\\000012.jpg', 'C:\\\\datasets\\\\pascal\\\\JPEGImages\\\\000017.jpg'\n",
    "img1, img2 = cv2.imread(img1_path), cv2.imread(img2_path)\n",
    "img1, img2 = cv2.resize(img1, (600, 300)), cv2.resize(img2, (600, 300))\n",
    "\n",
    "plt.imshow(img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
