{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm_notebook\n",
    "from matplotlib import pyplot as plt\n",
    "from itertools import product\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.functional import F\n",
    "from torchvision import models\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# from fastai import transforms, model, dataset, conv_learner\n",
    "\n",
    "from PIL import ImageDraw, ImageFont\n",
    "from matplotlib import patches, patheffects\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from augmentation import SSDAugmentation\n",
    "\n",
    "import Config\n",
    "\n",
    "torch.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kindeqi/PyTorch_SSD/dataset/VOCdevkit/VOC2007/JPEGImages/000012.jpg\n",
      "[[[67 69 69]\n",
      "  [64 66 66]\n",
      "  [65 67 67]\n",
      "  ...\n",
      "  [56 57 55]\n",
      "  [55 56 54]\n",
      "  [52 53 51]]\n",
      "\n",
      " [[70 72 72]\n",
      "  [67 69 69]\n",
      "  [65 67 67]\n",
      "  ...\n",
      "  [54 55 53]\n",
      "  [51 52 50]\n",
      "  [54 55 53]]\n",
      "\n",
      " [[63 65 65]\n",
      "  [65 67 67]\n",
      "  [66 68 68]\n",
      "  ...\n",
      "  [61 62 60]\n",
      "  [61 62 60]\n",
      "  [59 60 58]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[72 76 77]\n",
      "  [73 77 78]\n",
      "  [76 78 79]\n",
      "  ...\n",
      "  [82 84 84]\n",
      "  [83 85 85]\n",
      "  [82 84 84]]\n",
      "\n",
      " [[75 79 80]\n",
      "  [76 80 81]\n",
      "  [79 81 82]\n",
      "  ...\n",
      "  [76 78 78]\n",
      "  [77 79 79]\n",
      "  [77 79 79]]\n",
      "\n",
      " [[77 81 82]\n",
      "  [78 82 83]\n",
      "  [80 82 83]\n",
      "  ...\n",
      "  [76 81 80]\n",
      "  [77 79 79]\n",
      "  [77 79 79]]]\n",
      "This is test_dataset, total item num: 2501\n",
      "torch.Size([3, 300, 300]) (1, 4) (1,)\n"
     ]
    }
   ],
   "source": [
    "class VOC_dataset(Dataset):\n",
    "    def __init__(self, root_path, anno_path):\n",
    "        '''\n",
    "        Description:\n",
    "        Dataset\n",
    "\n",
    "        Arguments:\n",
    "        root_path: (str) the path to data directory ('C:\\\\datasets\\\\pascal\\\\') in this case\n",
    "        anno_path: (str) the path to annotation file ('C:\\\\datasets\\\\pascal\\\\PASCAL_VOC\\\\pascal_train2007.json') in this case\n",
    "        '''\n",
    "        self.dataset_json = json.load(open(anno_path))\n",
    "        self.img_path = root_path + '/JPEGImages/'\n",
    "        self.id_fname = {img['id']: img['file_name'] for img in self.dataset_json['images']}\n",
    "        self.id_list = [k for k in self.id_fname.keys()]\n",
    "    \n",
    "        self.id_annotation = defaultdict(list)\n",
    "        for anno in self.dataset_json['annotations']:\n",
    "            self.id_annotation[anno['image_id']].append([anno['bbox'], anno['category_id']])\n",
    "\n",
    "        self.id_single_anno = defaultdict(list)\n",
    "        for anno in self.id_annotation:\n",
    "            for bbox, c in self.id_annotation[anno]:\n",
    "                if self.id_single_anno[anno] == [] or bbox[-1] * bbox[-2] > self.id_single_anno[anno][0][0][-1] * self.id_single_anno[anno][0][0][-2]:\n",
    "                    self.id_single_anno[anno] = [[bbox, c]]\n",
    "\n",
    "        self.idx_category = {tmp['id']: tmp['name'] for tmp in self.dataset_json['categories']}\n",
    "        self.category_idx = {tmp['name']: tmp['id'] for tmp in self.dataset_json['categories']}\n",
    "\n",
    "        self.transforms = SSDAugmentation()\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        bbox, label = [], []\n",
    "        for anno in self.id_annotation[self.id_list[idx]]:\n",
    "            bbox.append(anno[0])\n",
    "            label.append(anno[1])\n",
    "            \n",
    "        img = cv2.imread(self.img_path + self.id_fname[self.id_list[idx]])\n",
    "        print(self.img_path + self.id_fname[self.id_list[idx]])\n",
    "        \n",
    "        img, bbox, label = np.float32(img), np.float32(bbox).reshape(-1, 4), np.int32(label)\n",
    "#         print(img.shape)\n",
    "        \n",
    "        img, bbox, label = self.transforms(img, bbox, label)\n",
    "\n",
    "        img = torch.tensor(img).permute(2, 0, 1)\n",
    "\n",
    "        return img, bbox, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset_json['images'])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    PATH = '/home/kindeqi/PyTorch_SSD/dataset/VOCdevkit/VOC2007'\n",
    "    anno_path = '/home/kindeqi/PyTorch_SSD/annotation/PASCAL_VOC/pascal_train2007.json'\n",
    "\n",
    "    test_dataset = VOC_dataset(root_path=PATH, anno_path=anno_path)\n",
    "    img, bbox, label = test_dataset[0]\n",
    "    print(\"This is test_dataset, total item num: {}\".format(len(test_dataset)))\n",
    "    print(img.shape, bbox.shape, label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[67 69 69]\n",
      "  [64 66 66]\n",
      "  [65 67 67]\n",
      "  ...\n",
      "  [56 57 55]\n",
      "  [55 56 54]\n",
      "  [52 53 51]]\n",
      "\n",
      " [[70 72 72]\n",
      "  [67 69 69]\n",
      "  [65 67 67]\n",
      "  ...\n",
      "  [54 55 53]\n",
      "  [51 52 50]\n",
      "  [54 55 53]]\n",
      "\n",
      " [[63 65 65]\n",
      "  [65 67 67]\n",
      "  [66 68 68]\n",
      "  ...\n",
      "  [61 62 60]\n",
      "  [61 62 60]\n",
      "  [59 60 58]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[72 76 77]\n",
      "  [73 77 78]\n",
      "  [76 78 79]\n",
      "  ...\n",
      "  [82 84 84]\n",
      "  [83 85 85]\n",
      "  [82 84 84]]\n",
      "\n",
      " [[75 79 80]\n",
      "  [76 80 81]\n",
      "  [79 81 82]\n",
      "  ...\n",
      "  [76 78 78]\n",
      "  [77 79 79]\n",
      "  [77 79 79]]\n",
      "\n",
      " [[77 81 82]\n",
      "  [78 82 83]\n",
      "  [80 82 83]\n",
      "  ...\n",
      "  [76 81 80]\n",
      "  [77 79 79]\n",
      "  [77 79 79]]]\n",
      "This is test_dataset, total item num: 2501\n",
      "torch.Size([3, 300, 300]) (1, 4) (1,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm_notebook\n",
    "from matplotlib import pyplot as plt\n",
    "from itertools import product\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.functional import F\n",
    "from torchvision import models\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# from fastai import transforms, model, dataset, conv_learner\n",
    "\n",
    "from PIL import ImageDraw, ImageFont\n",
    "from matplotlib import patches, patheffects\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from augmentation import SSDAugmentation\n",
    "\n",
    "torch.set_printoptions(precision=3)\n",
    "\n",
    "class VOC_dataset(Dataset):\n",
    "    def __init__(self, root_path, anno_path):\n",
    "        '''\n",
    "        Description:\n",
    "        Dataset\n",
    "\n",
    "        Arguments:\n",
    "        root_path: (str) the path to data directory ('C:\\\\datasets\\\\pascal\\\\') in this case\n",
    "        anno_path: (str) the path to annotation file ('C:\\\\datasets\\\\pascal\\\\PASCAL_VOC\\\\pascal_train2007.json') in this case\n",
    "        '''\n",
    "        self.dataset_json = json.load(open(anno_path))\n",
    "        self.img_path = root_path + '/JPEGImages/'\n",
    "        self.id_fname = {img['id']: img['file_name'] for img in self.dataset_json['images']}\n",
    "        self.id_list = [k for k in self.id_fname.keys()]\n",
    "    \n",
    "        self.id_annotation = defaultdict(list)\n",
    "        for anno in self.dataset_json['annotations']:\n",
    "            self.id_annotation[anno['image_id']].append([anno['bbox'], anno['category_id']])\n",
    "\n",
    "        self.id_single_anno = defaultdict(list)\n",
    "        for anno in self.id_annotation:\n",
    "            for bbox, c in self.id_annotation[anno]:\n",
    "                if self.id_single_anno[anno] == [] or bbox[-1] * bbox[-2] > self.id_single_anno[anno][0][0][-1] * self.id_single_anno[anno][0][0][-2]:\n",
    "                    self.id_single_anno[anno] = [[bbox, c]]\n",
    "\n",
    "        self.idx_category = {tmp['id']: tmp['name'] for tmp in self.dataset_json['categories']}\n",
    "        self.category_idx = {tmp['name']: tmp['id'] for tmp in self.dataset_json['categories']}\n",
    "\n",
    "        self.transforms = SSDAugmentation()\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        bbox, label = [], []\n",
    "        for anno in self.id_annotation[self.id_list[idx]]:\n",
    "            bbox.append(anno[0])\n",
    "            label.append(anno[1])\n",
    "            \n",
    "        img = cv2.imread(self.img_path + self.id_fname[self.id_list[idx]])\n",
    "        \n",
    "        print(self.img_path + self.id_fname[self.id_list[idx]])\n",
    "        \n",
    "        img, bbox, label = np.float32(img), np.float32(bbox).reshape(-1, 4), np.int32(label)\n",
    "        \n",
    "#         print(img.shape)\n",
    "        \n",
    "        img, bbox, label = self.transforms(img, bbox, label)\n",
    "\n",
    "        img = torch.tensor(img).permute(2, 0, 1)\n",
    "\n",
    "        return img, bbox, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset_json['images'])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    PATH = '/home/kindeqi/PyTorch_SSD/dataset/VOCdevkit/VOC2007'\n",
    "    anno_path = '/home/kindeqi/PyTorch_SSD/annotation/PASCAL_VOC/pascal_train2007.json'\n",
    "\n",
    "    test_dataset = VOC_dataset(root_path=PATH, anno_path=anno_path)\n",
    "    img, bbox, label = test_dataset[0]\n",
    "    print(\"This is test_dataset, total item num: {}\".format(len(test_dataset)))\n",
    "    print(img.shape, bbox.shape, label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
